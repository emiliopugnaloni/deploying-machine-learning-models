# ================
# 5.1 INTRODUCTION
# ================

# 1)ODD Files that we find
Manifest.in
mypy.ini
pyproject.toml
setup.py
tox.ini  -> it's where tox installs virtual envs

are things that are needed to build the package: configurations, dependencies, etc.

# 2) Requirements
we have a requierements directory where we formalize the dependencies of our package (requirements.txt)
and also for testing (test_requirements.txt)

# 3) Tests Directory
In the test drectory we have a couple of sample test

# 4)  Regression model directory
The regression model directory contains the mayority of the functionalities

# 5) Folder structure vs Jupyter Notebook 
Mapping between the .ipyb notebook and this package: We are not copy-pastyin the code however. It's more of a functionality mapping

# a) Import dependencies (at start of the notebook) 
-> Now is not at the start, it's when they are needed. It's not an individual modul with all the imports.

# b) Load the trainning data
-> it's now in regression-model/processing/data_manager.py. Here we have everything to load an save data

# c) Prep train/test data (test split section)
-> it's in train-pipeline (a bit odd, it's not in the processing module)

# d) Confis (parameters)
- > it's in 2 places. In regression-models/config.yaml and in regression-models/config/.. module. The last one is to perform some validations of the config. The "config" of the notebook is in the config.yaml

# e) Pipeliene (where the pipeline is set)
-> it's in the regression-models/pipeline.py

# f) Train the model (fit the pipeline)
-> it's in regression-models/train_pipelien.py

# g) Evaluate the model (preform predictions and evaluate the model on train & test)
-> it's in the regression-model/predict.py

# h) Persist the model (save the WHOLE pipeline as pkl)
-> it's in the regression-model/processing/data-manager.py

# i) Score New Data (predictions on new data)
-> it's made in test. And it will call for the pipeline, etc there

In the regression-model module we have three key files:
- pipeline.py: where we define the pipeline
- train_pipeline.py: where we fit the pipeline and save it
- predict.py: where we load the pipeline and make predictions

And then we have certain helpers: datasets we need (train and test), trained_models were we saved the models in pickle format to acces them in the future, processing, config


# ====================================================
# 5.2 Understand Reason Behind the Prod. code Structure
# ====================================================

# How do we know how to structure things and applying to our own project

There are 3 ways of kwnowing how to structure a project:
Convections
Packaging mandatory files
Software engineering best practices

# 1) Conventions: 
    - Versioning
    - Config
    - Pep8/linting tools

There are things that won't throw an error, but are a standard in the industry.
The most obvious is PEP8 (provides lot of gidelines)
In python ther are different ways of doing versioning. We have the VERSION file that is one.
About config, we have the config.yaml file, that is read by a config python module (config/core.py)

# 2) Packaging mandatory files
Once we decided that we are going to pakcage our module in a Python package, then there are certain file required to do that:
    - setup.py
    - manifest.in

Â¿Why are we packaging our model? Review section 3, but boradly speaking, we are doing it to be able to embeded it in applications. There are other ways of persist and share our models, but this is the most common one.

# 3) Software Engineering Best Practices

# a) For Packages
If we think about the end user, it's going to be used to train and predict. That's why we separate train and predict

# b) General
    - Testing: Can you write a single unit test for one specific bit of functionality. For example, the data_manager.py: we have functions to save-pipeline and removing-pipeline. They could be a single function, but now we can test saving and removing the pipeline separately
    - Separations of Concerns -> Modularity: It's about having a single module that does one thing. For example, the data_manager.py is about loading and saving data. The validation.py is about testist that the inputs are correct. We choose to separete that because they do diff things, and that leads to testing
    - SOLID principles: The principal is "Single Responsibility Principle". It means that a class or module should have one and only one job. That is whay we tray to do in predict.py (dealing with predict), ...
    - Maintainability: The code is easy to maintain. For example, if we want to change the way we save the pipeline, we only need to change it in one place (data_manager.py). If we had all the code in one file, it would be harder to maintain.
    - Optimize the code for redability (this is key also)


# ==============================
# 5.3 Package Requirements File
# ==============================

# We define the compatible version of the packages we ae using. The version follows this structure: Major_Version.Minor_Version.Patch_Version. The major_version changes when it has an update that breaks the previous api

# We can install all of this with: pip install -r requirements\requirements.txt
# We can also install the test requirements with: pip install -r requirements\test_requirements.txt

# This are separated because the test req are not needed every time
